{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Training the model\n",
    "Introduction: In this part, we will load the balance data we achieved in part 2, and use it to train a decision tree model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert .pkl data into a data frame, we uuse '.read_pickle'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SK_ID_CURR  TARGET NAME_CONTRACT_TYPE CODE_GENDER FLAG_OWN_CAR  \\\n",
      "1      100003       0         Cash loans           F            N   \n",
      "2      100004       0    Revolving loans           M            Y   \n",
      "3      100006       0         Cash loans           F            N   \n",
      "4      100007       0         Cash loans           M            N   \n",
      "5      100008       0         Cash loans           M            N   \n",
      "\n",
      "  FLAG_OWN_REALTY  CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  AMT_ANNUITY  \\\n",
      "1               N             0          270000.0   1293502.5      35698.5   \n",
      "2               Y             0           67500.0    135000.0       6750.0   \n",
      "3               Y             0          135000.0    312682.5      29686.5   \n",
      "4               Y             0          121500.0    513000.0      21865.5   \n",
      "5               Y             0           99000.0    490495.5      27517.5   \n",
      "\n",
      "              ...              FLAG_DOCUMENT_18 FLAG_DOCUMENT_19  \\\n",
      "1             ...                             0                0   \n",
      "2             ...                             0                0   \n",
      "3             ...                             0                0   \n",
      "4             ...                             0                0   \n",
      "5             ...                             0                0   \n",
      "\n",
      "  FLAG_DOCUMENT_20 FLAG_DOCUMENT_21 AMT_REQ_CREDIT_BUREAU_HOUR  \\\n",
      "1                0                0                        0.0   \n",
      "2                0                0                        0.0   \n",
      "3                0                0                        NaN   \n",
      "4                0                0                        0.0   \n",
      "5                0                0                        0.0   \n",
      "\n",
      "  AMT_REQ_CREDIT_BUREAU_DAY  AMT_REQ_CREDIT_BUREAU_WEEK  \\\n",
      "1                       0.0                         0.0   \n",
      "2                       0.0                         0.0   \n",
      "3                       NaN                         NaN   \n",
      "4                       0.0                         0.0   \n",
      "5                       0.0                         0.0   \n",
      "\n",
      "   AMT_REQ_CREDIT_BUREAU_MON  AMT_REQ_CREDIT_BUREAU_QRT  \\\n",
      "1                        0.0                        0.0   \n",
      "2                        0.0                        0.0   \n",
      "3                        NaN                        NaN   \n",
      "4                        0.0                        0.0   \n",
      "5                        0.0                        1.0   \n",
      "\n",
      "   AMT_REQ_CREDIT_BUREAU_YEAR  \n",
      "1                         0.0  \n",
      "2                         0.0  \n",
      "3                         NaN  \n",
      "4                         0.0  \n",
      "5                         1.0  \n",
      "\n",
      "[5 rows x 122 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "unpickled_df = pd.read_pickle('balancedData.pkl')\n",
    "print(unpickled_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non- numeric data - string in the data frame\n",
    "Looking at the data above, reveals some of the columns have non-numeric values, e.g., CODE_GENDER contains F or M values.\n",
    "Before appliying Decision Tree, we need to transform the non-numeric vaalues to numeric ones. We need a dictionary to uniquely assign numeric vaues to non-numeric ones in each column. defaultdict(LabelEncoder) defines the dictionary d.\n",
    "In the next line, we convert all strings into numeric values using the defined dictionart, d. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "d = defaultdict(LabelEncoder)\n",
    "numeric_train_df = unpickled_df.apply(lambda x: d[x.name].fit_transform(x.astype(str)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting inputs (features) and outputs (targets) for our model\n",
    "To define a model based on the historical data we have from the balanced training set, we shall determine inputs and outputs. We then select the TARGET values from the numeric_train_df and call it y.\n",
    "We select all columns except the target and the applicant's ID as training features (Input of our model) and name it X.\n",
    "We then select the TARGET values from the numeric_train_df and call it y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_of_interest =(numeric_train_df.drop(['TARGET','SK_ID_CURR'], axis=1)).columns\n",
    "X = numeric_train_df [columns_of_interest]\n",
    "y = numeric_train_df.TARGET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the training data\n",
    "Now that X, and Y are ready, we can apply any machine learning algorithm we like, and train a model to describe the relation between inputs and outputs in our training set. But wait! We need to keep some of the training data as a validation set to check the performance of our model. Therefore, we use train_test_split to split the data into training and validating data. By default 25% of the training data is kept aside for validation. However, we can change this percentage by changing the value of test_size in the train_test_split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y,test_size = 0.25, random_state = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train_X, and train_y contains features and targets of the training set, and val_X, and val_y have validation set values.\n",
    "We have used .shape to have an idea of the size of the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Features Shape:', train_X.shape)\n",
    "print('Training Labels Shape:', train_y.shape)\n",
    "print('Validation Features Shape:', val_X.shape)\n",
    "print('Validation Labels Shape:', val_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Descision Tree Model to the training data:\n",
    "Luckily, we can easilyimport the desired machine learning library, RandomForestClassifier here, and use it to train our model. In the first step, we initiate the model structureand name it clf. By convention, clf means 'Classifier', and in the next step, we train the Classifier to take the training features and learn how they relate to the training y using '.fit'. This instruction may take a few minutes or more to find the best model to fint the training data. As a result, the specifications of the decision tree would appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Method ='RandomForestClassifier' # We will uuse this name later when exporting the results\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "clf = RandomForestClassifier(class_weight=\"balanced\", max_features = 11)\n",
    "clf.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Predictions on the Validation Set:\n",
    "Now that the clf is ready, we can predict the output of a given set of features (inputs) by using'.predict' method. Weapply it on the validation set to predict the output of the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(val_X)\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Classifier\n",
    "We have predicted the output values (targets) for the validation data. Now, we can compare it with the real outputs we have and calculate the error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "error = predictions - val_y\n",
    "print('Mean Error:', round(np.mean(error), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "print('Mean Absolute Error = %.2f'% (100*mean_absolute_error(val_y, predictions)),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error is arround 0.6%. This is a good indicator of total error we have from the validation set. However, it does not judge the performance of th model on targets and non-targets. There are other indicators such as cv_score, and F1 score which can better judge about the performance of the trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "cv_scores = log_loss(val_y, predictions)\n",
    "print('cv_scores =', cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the model performance: False and true predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=pd.DataFrame({'Validation':val_y,'Predictions':predictions, 'Error': error})\n",
    "result.describe()\n",
    "#print(result)\n",
    "print('Number of false predictions:', result['Error'].abs().sum())\n",
    "print('Number of total predictions:', result['Error'].count())\n",
    "print('Number of correct predictions:', result['Error'].abs().count()-result['Error'].sum())\n",
    "fp = (result['Error']==1).sum()\n",
    "fn = (result['Error']==-1).sum()\n",
    "tp = ((result['Predictions']==1)&(result['Validation']==1)).sum()\n",
    "tn = ((result['Predictions']==0)&(result['Validation']==0)).sum()\n",
    "print('Number of false positives:',fp )\n",
    "print('Number of false negatives:',fn)\n",
    "print('Number of true positives:', tp)\n",
    "print('Number of true negatives:', tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Popular metrics: F1 score, prescision, recall, and accuracy: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = round((tp*100)/(tp+fp),2)\n",
    "print('Precision =', precision,'%')\n",
    "recall = round((tp*100)/(tp+fn),2)\n",
    "print('Recall = %.2f'%recall,'%')\n",
    "F1 = round(2*(precision*recall)/(precision+recall),2)\n",
    "print('F1 score = %.2f' %F1, '%')\n",
    "accuracy = round(100*(tp +tn)/(tp+tn+fp+fn),2)\n",
    "print('Accuracy = %.2f' %accuracy, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recall increased to almost 100% with a high accuracy and F1 score. There is no false negatives which is very good. It means, all of the applicants whom we predict to return their loan, will do.\n",
    "There are a few applicant whom we predict not to return their loan, but they will. So, the model is a bit conservative in determining the trustable applicants which is good. We can distinguish all applicants who will not return their loan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the TARGET column of the Test data sets\n",
    "Now that the model is ready, and the performance is acceptable, we predict the targets of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('Data/application_test.csv')\n",
    "Test_columns_of_interest =(df_test.drop(['SK_ID_CURR'], axis=1)).columns\n",
    "Test_X = df_test[Test_columns_of_interest]\n",
    "\n",
    "p = clf.predict(Test_X.apply(lambda x: d[x.name].fit_transform(x.astype(str))))\n",
    "f_result=pd.DataFrame({'SK_ID_CURR':df_test['SK_ID_CURR'],'TARGET':p})\n",
    "my_result= f_result.set_index('SK_ID_CURR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('We predict', f_result['TARGET'].sum(),'number of applicants out of', f_result['TARGET'].count(),'applicants in total, will not repay their loan.' )\n",
    "ratio = f_result['TARGET'].sum()/f_result['TARGET'].count()*100\n",
    "print('In the other words, base on the historical data given, %.2f'%ratio, '% of applicants are predicted not to repay their loans.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export the result to SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import re\n",
    "conn = sqlite3.connect('Summary.sqlite')\n",
    "my_result.to_sql('MyResult', conn, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is next? \n",
    "We need to do some try and error to find the best model, and tune some parameters, e.g., training with original unbalanced data sets and so on. The best way to compare different models is to extract the performance metrics of each method in a table and compare them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = conn.cursor()\n",
    "cur.execute('DROP TABLE IF EXISTS Results')\n",
    "cur.execute('''\n",
    "CREATE TABLE Results (Method TEXT, Accuracy FLOAT, Precision FLOAT, F1 FLOAT, Recall FLOAT)''')\n",
    "cur.execute('''INSERT INTO Results (Method, Accuracy, Precision, F1, Recall)\n",
    "               VALUES (?, ?, ?, ?, ?)''', (Method, accuracy, precision, F1, recall))\n",
    "conn.commit()\n",
    "cur.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
