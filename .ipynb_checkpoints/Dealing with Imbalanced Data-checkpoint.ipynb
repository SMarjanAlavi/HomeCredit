{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HomeCredit \n",
    "# Part 2: Dealing with Imbalance Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction:\n",
    "\n",
    "Before we apply any machine learning algorithm to predict the target of the test set, we need to see if the training set is balanced or not. If the training set is imbalance, the prediction quality drops significantly. In most of the cases like in this project, the training set is imbalanced; we have more information about the applicants who have returned their loan, and we have only few examples of the applicants who did not repaid their loan. So, if we work with this imbalance set, the prediction behaves better of the applicants with TARGET = 0, and it will face problems (higher error) on determining the applicants who will not repay their loan.\n",
    "\n",
    "So, the first task is to find how imbalance our training set is. To find the number of Targets, we can use '.sum()' attribute. Note that we cannot use the '.count()' attribute here, because it will count both zeros and ones in the TARGET column.\n",
    "The difference between the number of tatal rows in the TARGET column and the number of the TARGETS with the value of one, gives us the number of non-target samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.read_csv('Data/application_train.csv')\n",
    "#df_test = pd.read_csv('Data/application_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 307511 samples; 24825 of them with TARGETS = 1, and  282686 with TARGET= 0. So, only 8.1 % of samples are targets.\n"
     ]
    }
   ],
   "source": [
    "number_of_targets = df_train[\"TARGET\"].sum()\n",
    "number_of_samples = df_train['SK_ID_CURR'].count()\n",
    "number_of_non_targets = number_of_samples - number_of_targets\n",
    "balance_ratio = number_of_targets / number_of_samples * 100\n",
    "print('We have', number_of_samples, 'samples;', number_of_targets,'of them with TARGETS = 1, and ',\\\n",
    "      number_of_non_targets, 'with TARGET= 0. So, only %.1f'%balance_ratio, '% of samples are targets.' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancing the Data: Oversampling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to achieve a balances training set, is to repeat some of the training samples with TARGET = 1 such that the number of samples with TARGET = 1 become equal to the number of TARGET = 0.\n",
    "Now, we calculate 'Oversampling_gain' which indicates how many time the Target samples need to be repeated to achieve the balance data set. 'int' converts the result into an integer value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In order to achieve a 1:1 ratio balanced dataset we will add 257861 extra targets to the training set.\n",
      "Oversampling gain =  12\n"
     ]
    }
   ],
   "source": [
    "extra_targets = number_of_non_targets - number_of_targets\n",
    "print('In order to achieve a 1:1 ratio balanced dataset we will add', extra_targets, 'extra targets to the training set.')\n",
    "Oversampling_gain = int((extra_targets + number_of_targets) / number_of_targets)+1\n",
    "print('Oversampling gain = ', Oversampling_gain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's copy samples with target = 1 from the training set and add them to the training set to achieve a balanced training data.\n",
    "So, first we will separate rowswoth TARGET = 1 and save them in a new data frame and name it 'train_ones'. We put the rest of the rows in 'train_zeros' data frame.\n",
    "df_train['TARGET'].isin(['1']) checks weather the the TARGET values are equal to '1' or not, the result of this comparision is either True, or False. When we put this expression inside brackets in front of a column name, the row which refers to the True values are selected and the other ones are deleted from the new datafarem. We can find the 'train_zeros' dataframe similarly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_ones = df_train[df_train['TARGET'].isin(['1'])]\n",
    "df_train_zeros = df_train[df_train['TARGET'].isin(['0'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'.concat' repeats a dataframe to make a new bigger dataframe out of itself (self-concatenating). Here, the 'df_train_ones' is concatenating with itself for 'Oversampling_gain = 12' times to achieve the extra TARGETS. In the second line, we add the zero target samples to the dataframe with extra targets to achieve the balanced data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_train_ones' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-d2cfc107dbda>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mOversampling_gain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m12\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf_train_ones_oversampled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_train_ones\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mOversampling_gain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdf_train_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_train_zeros\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf_train_ones_oversampled\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train_ones\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_train_ones' is not defined"
     ]
    }
   ],
   "source": [
    "df_train_ones_oversampled = pd.concat([df_train_ones]*Oversampling_gain)\n",
    "df_train_samples = pd.concat([df_train_zeros,df_train_ones_oversampled])\n",
    "print(len(df_train_ones))\n",
    "print(len(df_train_ones_oversampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import plot\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "#from lightgbm import LGBMClassifier\n",
    "#import lightgbm as lgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', UserWarning)\n",
    "\n",
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Descision Tree Model to the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = defaultdict(LabelEncoder)\n",
    "mydf = train_samples\n",
    "my_data = mydf.apply(lambda x: d[x.name].fit_transform(x.astype(str)))\n",
    "y = my_data.TARGET\n",
    "# Let's select all columns except the target and the applicant's ID as training features.\n",
    "columns_of_interest =(my_data.drop(['TARGET','SK_ID_CURR'], axis=1)).columns\n",
    "X = my_data [columns_of_interest]\n",
    "#print('Columns used as training features are: \\n', X)\n",
    "#X.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# We split the training set to use some of the samples for validating our model.\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y,test_size = 0.25, random_state = 20)\n",
    "# Define model\n",
    "print('Training Features Shape:', train_X.shape)\n",
    "print('Training Labels Shape:', train_y.shape)\n",
    "print('Validation Features Shape:', val_X.shape)\n",
    "print('Validation Labels Shape:', val_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "# Create a random forest Classifier. By convention, clf means 'Classifier'\n",
    "#clf = RandomForestClassifier(n_jobs=2, random_state=0, class_weight=\"balanced\")\n",
    "clf = RandomForestClassifier(class_weight=\"balanced\", max_features = 11)\n",
    "\n",
    "# Train the Classifier to take the training features and learn how they relate to the training y.\n",
    "clf.fit(train_X, train_y)\n",
    "#clf.fit(train_X, np.ones(len(train_X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Predictions on the Validation Set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the forest's predict method on the test data\n",
    "predictions = clf.predict(val_X)\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Calculate the absolute errors\n",
    "error = predictions - val_y\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Error:', round(np.mean(error), 4))\n",
    "val_y.shape\n",
    "cv_scores = log_loss(val_y, predictions)\n",
    "print('cv_scores =', cv_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets analyse the result:\n",
    "result=pd.DataFrame({'Validation':val_y,'Predictions':predictions, 'Error': error})\n",
    "result.describe()\n",
    "#print(result)\n",
    "print('Number of false predictions:', result['Error'].abs().sum())\n",
    "print('Number of total predictions:', result['Error'].count())\n",
    "print('Number of correct predictions:', result['Error'].abs().count()-result['Error'].sum())\n",
    "fp = (result['Error']==1).sum()\n",
    "fn = (result['Error']==-1).sum()\n",
    "tp = ((result['Predictions']==1)&(result['Validation']==1)).sum()\n",
    "tn = ((result['Predictions']==0)&(result['Validation']==0)).sum()\n",
    "print('Number of false positives:',fp )\n",
    "print('Number of false negatives:',fn)\n",
    "print('Number of true positives:', tp)\n",
    "print('Number of true negatives:', tn)\n",
    "print('Mean Absolute Error = %.2f'% (100*mean_absolute_error(val_y, predictions)),'%')\n",
    "precision = round((tp*100)/(tp+fp),2)\n",
    "print('Precision =', precision,'%')\n",
    "recall = round((tp*100)/(tp+fn),2)\n",
    "print('Recall = %.2f'%recall,'%')\n",
    "F1 = round(2*(precision*recall)/(precision+recall),2)\n",
    "print('F1 score = %.2f' %F1, '%')\n",
    "accuracy = round(100*(tp +tn)/(tp+tn+fp+fn),2)\n",
    "print('Accuracy = %.2f' %accuracy, '%')\n",
    "\n",
    "#Kappa (or Cohen’s kappa): Classification accuracy normalized by the imbalance of the classes in the data.\n",
    "#ROC Curves:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the Test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print(val_X.head(1))\n",
    "#print(test.shape)\n",
    "#test_X = val_X.iloc[1:2, :]\n",
    "#print(test.head())\n",
    "Test_columns_of_interest =(test.drop(['SK_ID_CURR'], axis=1)).columns\n",
    "Test_X = test[Test_columns_of_interest]\n",
    "\n",
    "p = clf.predict(Test_X.apply(lambda x: d[x.name].fit_transform(x.astype(str))))\n",
    "f_result=pd.DataFrame({'SK_ID_CURR':test['SK_ID_CURR'],'TARGET':p})\n",
    "my_result= f_result.set_index('SK_ID_CURR')\n",
    "\n",
    "#print(p.shape)\n",
    "#print('My result = ',my_result)\n",
    "#print('Test Index =\\n', test_X.index)\n",
    "#test.index.shape\n",
    "#print('Actual Target Value =\\n',val_y[test_X.index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export the result to SQL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import re\n",
    "conn = sqlite3.connect('Summary.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('DROP TABLE IF EXISTS Results')\n",
    "\n",
    "cur.execute('''\n",
    "CREATE TABLE Results (Method TEXT, Accuracy FLOAT, Precision FLOAT, F1 FLOAT, Recall FLOAT)''')\n",
    "cur.execute('''INSERT INTO Results (Method, Accuracy, Precision, F1, Recall)\n",
    "               VALUES (?, ?, ?, ?, ?)''', (Method, accuracy, precision, F1, recall))\n",
    "conn.commit()\n",
    "cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f_result['TARGET'].sum())\n",
    "print(f_result['TARGET'].count())\n",
    "f_result['TARGET'].sum()/f_result['TARGET'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "The recall increased to almost 100% with a high accuracy and F1 score. There is no false negatives which is very good. It means, all of the applicants whom we predict to return their loan, will do.\n",
    "There are a few applicant whom we predict not to return their loan, but they will. So, the model is a bit conservative in determining the trustable applicants which is good. We can distinguish all applicants who will not return their loan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sqlalchemy import create_engine\n",
    "#engine = create_engine('sqlite://', echo=False)\n",
    "my_result.to_sql('MyResult2', conn, if_exists='replace')\n",
    "#engine.execute(\"SELECT * FROM FinalResults\").fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
